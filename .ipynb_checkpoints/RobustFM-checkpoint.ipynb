{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-24T10:21:31.871049Z",
     "start_time": "2017-06-24T10:21:28.918804Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "\n",
    "import warnings\n",
    "from sklearn.externals.joblib import Memory\n",
    "from sklearn.datasets import load_svmlight_file\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "mem = Memory('./mycache')\n",
    "\n",
    "# The Memory class defines a context for lazy evaluation of function, by storing the results to the disk, \n",
    "# and not rerunning the function twice for the same arguments.\n",
    "# It works by explicitly saving the output to a file and it is designed to work with non-hashable and potentially large input and output data types such as numpy arrays.\n",
    "@mem.cache\n",
    "def get_data(file_path):\n",
    "    data = load_svmlight_file(file_path)\n",
    "    return data\n",
    "\n",
    "# classification task\n",
    "# a1a\n",
    "a1a_train = 'data/a1a/a1a.train'\n",
    "a1a_test = 'data/a1a/a1a.test'\n",
    "# a8a\n",
    "a8a_train = 'data/a8a/a8a.train'\n",
    "a8a_test = 'data/a8a/a8a.test'\n",
    "#ijcnn\n",
    "ijcnn_train = 'data/ijcnn/ijcnn.train'\n",
    "ijcnn_test = 'data/ijcnn/ijcnn.test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-24T10:27:10.279805Z",
     "start_time": "2017-06-24T10:27:09.881114Z"
    },
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fm(train_x, train_y, test_x, test_y, training_config):\n",
    "    # pack parameters\n",
    "    learning_rate = training_config['learning_rate'] if 'learning_rate' in training_config else 1e4\n",
    "    t0 = training_config['t0'] if 't0' in training_config else 1e5\n",
    "    skip = training_config['skip'] if 'skip' in training_config else 1e3\n",
    "    factor = training_config['factor'] if 'factor' in training_config else 10\n",
    "    task = training_config['task_type'] if 'task_type' in training_config else 'regression'\n",
    "    \n",
    "    if task not in ['classification', 'regression']:\n",
    "        raise Exception('unsupported task type')\n",
    "    \n",
    "    epoch = training_config['epoch'] if 'epoch' in training_config else 10\n",
    "    verbose = training_config['verbose'] if 'verbose' in training_config else True\n",
    "    is_shuffle = training_config['shuffle'] if 'shuffle' in training_config else True\n",
    "    \n",
    "    seed = training_config['seed'] if 'seed' in training_config else 1\n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    training_num, p = train_x.shape\n",
    "    test_num, p_test = test_x.shape\n",
    "    \n",
    "    if p != p_test:\n",
    "        warnings.warn ('different dimensions in test and train data')\n",
    "        p = max(p, p_test)\n",
    "        \n",
    "    # mandatory shuffle at the beginning\n",
    "#     x_train, y_train = shuffle(train_x, train_y, random_state=seed)\n",
    "    x_train, y_train = train_x, train_y\n",
    "    \n",
    "    train_loss_epochwise = []\n",
    "    test_loss_epochwise = []\n",
    "    \n",
    "    count = skip\n",
    "    \n",
    "    # initialization\n",
    "    w0 = 0;\n",
    "    W = np.zeros([1,p])\n",
    "#     V = 0.1 * np.random.randn(p, factor)\n",
    "    V = np.zeros([p, factor])\n",
    "        \n",
    "    for i in range(epoch):\n",
    "        \n",
    "        # if shuffle is true, do shuffle each epoch\n",
    "        if is_shuffle:\n",
    "            x_train, y_train = shuffle(train_x, train_y, random_state=seed)\n",
    "        \n",
    "        loss = 0\n",
    "        for j in range(training_num):\n",
    "            \n",
    "            x = x_train[j]\n",
    "            y = y_train[j]\n",
    "            nz_idx = np.nonzero(x)[1]\n",
    "            \n",
    "            x = x[np.nonzero(x)]\n",
    "            tmp = np.sum(np.multiply(np.matlib.repmat(x.T,1,factor),V[nz_idx,:]),axis=0);\n",
    "            factor_part = (np.sum(np.square(tmp)) - np.sum(np.multiply(np.matlib.repmat(np.square(x.T),1,factor),np.square(V[nz_idx,:]))))/2\n",
    "            y_predict = (w0 + np.dot(W[:,nz_idx],x.T) + factor_part).item()\n",
    "            \n",
    "            if task == 'classification':\n",
    "                err = 1/(1+np.exp(-y_predict*y))\n",
    "                loss += (-np.log(err))\n",
    "            \n",
    "            if task == 'regression':\n",
    "                err = y_predict - y\n",
    "                loss += err**2\n",
    "            \n",
    "            t = training_num * i + j\n",
    "            # update\n",
    "            if task == 'classification':\n",
    "                w0_ = learning_rate / (t0 + t) * (err - 1) * y\n",
    "                w0 -= w0_\n",
    "                \n",
    "                W_ = learning_rate / (t0 + t) * (err - 1) * y * x\n",
    "                W[:,nz_idx] -= W_\n",
    "                \n",
    "                V_ = learning_rate / (t0 + t) * (err - 1) * y *\\\n",
    "                np.multiply(\n",
    "                    np.matlib.repmat(x.T,1,factor),(\n",
    "                        np.matlib.repmat(np.dot(x,V[nz_idx,:]),len(nz_idx),1)-\\\n",
    "                        np.multiply(np.matlib.repmat(x.T,1,factor),V[nz_idx,:])\n",
    "                    )\n",
    "                )\n",
    "                V[nz_idx,:] -= V_\n",
    "            if task == 'regression':\n",
    "                w0_ = learning_rate / (t0 + t) * 2 * err\n",
    "                w0 -= w0_\n",
    "                \n",
    "                W_ = learning_rate / (t0 + t) * 2 * err * x\n",
    "                W[:,nz_idx] -= W_\n",
    "                \n",
    "                V_ = learning_rate / (t0 + t) * 2 * err *\\\n",
    "                np.multiply(\n",
    "                    np.matlib.repmat(x.T,1,factor),(\n",
    "                        np.matlib.repmat(np.dot(x,V[nz_idx,:]),len(nz_idx),1)-\\\n",
    "                        np.multiply(np.matlib.repmat(x.T,1,factor),V[nz_idx,:])\n",
    "                    )\n",
    "                )\n",
    "                V[nz_idx,:] -= V_\n",
    "                \n",
    "            count -= 1\n",
    "            if count <= 0:\n",
    "                W *= (1 - skip/(t + t0))\n",
    "                V *= (1 - skip/(t + t0))\n",
    "                count = skip\n",
    "        \n",
    "        train_loss_epochwise.append(loss/training_num)\n",
    "        \n",
    "        if verbose:\n",
    "            print('epoch[%d]---train loss: %.4f\\t' % (i,loss/training_num))\n",
    "            \n",
    "        loss = 0   \n",
    "        # validation\n",
    "        for j in range(test_num):\n",
    "            x = test_x[j]\n",
    "            y = test_y[j]\n",
    "            nz_idx = np.nonzero(x)[1]\n",
    "            \n",
    "            x = x[np.nonzero(x)]\n",
    "            tmp = np.sum(np.multiply(np.matlib.repmat(x.T,1,factor),V[nz_idx,:]),axis=0);\n",
    "            factor_part = (np.sum(np.square(tmp)) - np.sum(np.multiply(np.matlib.repmat(np.square(x.T),1,factor),np.square(V[nz_idx,:]))))/2\n",
    "            y_predict = (w0 + np.dot(W[:,nz_idx],x.T) + factor_part).item()\n",
    "            \n",
    "            if task == 'classification':\n",
    "                err = 1/(1+np.exp(-y_predict*y))\n",
    "                loss += (-np.log(err))\n",
    "            \n",
    "            if task == 'regression':\n",
    "                err = y_predict - y\n",
    "                loss += err**2\n",
    "                \n",
    "        test_loss_epochwise.append(loss/test_num)\n",
    "        \n",
    "        if verbose:\n",
    "            print('test loss: %.4f\\n' % (loss/test_num))\n",
    "        \n",
    "    return train_loss_epochwise, test_loss_epochwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-24T10:28:40.645669Z",
     "start_time": "2017-06-24T10:27:12.355214Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch[0]---train loss: 0.2257\t\n",
      "test loss: 0.2992\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_data = ijcnn_train\n",
    "test_data = ijcnn_test\n",
    "\n",
    "train_x, train_y = get_data(train_data)\n",
    "test_x, test_y = get_data(test_data)\n",
    "\n",
    "training_config = {\n",
    "    'learning_rate':1e4,\n",
    "    't0':1e5,\n",
    "    'skip':1e3,\n",
    "    'task_type':'classification',\n",
    "    'factor': 10,\n",
    "    'verbose': True,\n",
    "    'epoch': 1,\n",
    "    'shuffle': False,\n",
    "    'seed': 2017\n",
    "}\n",
    "fm(train_x, train_y, test_x, test_y, training_config);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
